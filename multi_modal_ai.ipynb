{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSkLUBT-yo0W",
        "outputId": "dffe1537-3987-4736-9fbf-caf80336d8fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import time\n",
        "\n",
        "API_KEY = \"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\"\n",
        "client = OpenAI(api_key=API_KEY)\n"
      ],
      "metadata": {
        "id": "8w7EuqU3zvMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'intent': {\n",
        "        'model': \"gpt-3.5-turbo-0125\",\n",
        "        'system_prompt': \"\"\"You are a helpful assistant.\n",
        "Classify the following prompt as a 'simple' or 'hard' task.\n",
        "The task is 'hard' only if it requires background in computer science.\n",
        "Return just one lower-case word: 'simple' or 'hard':\"\"\",\n",
        "        'max_tokens': 5,\n",
        "        'price_input': 0.5,\n",
        "        'price_output': 1.5\n",
        "    },\n",
        "\n",
        "    'easy': {\n",
        "        'model': \"gpt-3.5-turbo-0125\",\n",
        "        'system_prompt': \"You're a helpful assistant\",\n",
        "        'max_tokens': None,\n",
        "        'price_input': 0.5,\n",
        "        'price_output': 1.5\n",
        "    },\n",
        "\n",
        "    'hard': {\n",
        "        'model': \"gpt-4o\",\n",
        "        'system_prompt': \"You're a helpful assistant\",\n",
        "        'max_tokens': None,\n",
        "        'price_input': 5,\n",
        "        'price_output': 15\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "SnAkRSdvz_Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_model(prompt, model_type):\n",
        "    start_time = time.time()\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=config[model_type]['model'],\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": config[model_type]['system_prompt']},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=config[model_type]['max_tokens']\n",
        "    )\n",
        "\n",
        "    latency = time.time() - start_time\n",
        "\n",
        "    # Определяем intent, только если это классификатор\n",
        "    if model_type == 'intent':\n",
        "        intent = response.choices[0].message.content\n",
        "    else:\n",
        "        intent = None\n",
        "\n",
        "    # Подсчёт токенов\n",
        "    tokens_input = response.usage.prompt_tokens\n",
        "    tokens_output = response.usage.completion_tokens\n",
        "\n",
        "    # Стоимость = токены * коэффициент модели\n",
        "    price = (\n",
        "        tokens_input * config[model_type]['price_input']\n",
        "        + tokens_output * config[model_type]['price_output']\n",
        "    )\n",
        "\n",
        "    print(response.choices[0].message.content)  # вывод ответа модели\n",
        "\n",
        "    return price, latency, intent\n"
      ],
      "metadata": {
        "id": "yJR4LwmZ0IAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EASY_QUERY = 'Who is the best actor who played spider-man?'\n",
        "HARD_QUERY = 'What is the difference between Adam and AdamW optimizers?'\n",
        "\n",
        "print(\"Hard Query intent:\")\n",
        "call_model(HARD_QUERY, 'intent')\n",
        "\n",
        "print(\"\\nEasy Query intent:\")\n",
        "call_model(EASY_QUERY, 'intent')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "QUcy-r0l0Odz",
        "outputId": "fec1a6c1-b518-4c26-cd06-4ee0b6d29642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hard Query intent:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1617582774.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hard Query intent:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcall_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHARD_QUERY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'intent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEasy Query intent:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2778305900.py\u001b[0m in \u001b[0;36mcall_model\u001b[0;34m(prompt, model_type)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "\n",
        "def mock_response(model_type, prompt):\n",
        "    \"\"\"\n",
        "    Имитация поведения модели в зависимости от типа.\n",
        "    \"\"\"\n",
        "\n",
        "    # Имитация задержки ответа\n",
        "    latency = random.uniform(0.05, 0.25)\n",
        "\n",
        "    # Имитация количества токенов\n",
        "    tokens_input = random.randint(5, 20)\n",
        "    tokens_output = random.randint(8, 40)\n",
        "\n",
        "    # Имитация классификатора intent\n",
        "    if model_type == \"intent\":\n",
        "        if \"optimizer\" in prompt.lower() or \"adam\" in prompt.lower():\n",
        "            content = \"hard\"\n",
        "        else:\n",
        "            content = \"simple\"\n",
        "        intent = content\n",
        "    else:\n",
        "        content = f\"[MOCK RESPONSE from {model_type.upper()} model]\"\n",
        "        intent = None\n",
        "\n",
        "    # Цена (как в конфиге)\n",
        "    price = tokens_input * config[model_type]['price_input'] + \\\n",
        "            tokens_output * config[model_type]['price_output']\n",
        "\n",
        "    return content, latency, intent, tokens_input, tokens_output, price\n",
        "\n",
        "\n",
        "def call_model(prompt, model_type):\n",
        "    \"\"\"\n",
        "    Обёртка над mock-моделью — имитируем API.\n",
        "    \"\"\"\n",
        "\n",
        "    content, latency, intent, tokens_input, tokens_output, price = mock_response(model_type, prompt)\n",
        "\n",
        "    print(content)  # выводим имитированный ответ\n",
        "    return price, latency, intent\n"
      ],
      "metadata": {
        "id": "pFxdvzLm0waP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EASY_QUERY = \"Who is the best actor who played spider-man?\"\n",
        "HARD_QUERY = \"What is the difference between Adam and AdamW optimizers?\"\n",
        "\n",
        "print(\"Hard Query intent:\")\n",
        "call_model(HARD_QUERY, \"intent\")\n",
        "\n",
        "print(\"\\nEasy Query intent:\")\n",
        "call_model(EASY_QUERY, \"intent\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fdhM8gV030H",
        "outputId": "653e9d46-66cb-44fc-c2cd-dc6f1aaf7d3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hard Query intent:\n",
            "hard\n",
            "\n",
            "Easy Query intent:\n",
            "simple\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(33.5, 0.06216880217607104, 'simple')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Using ONLY powerful model (gpt-4o mock):\\n\")\n",
        "\n",
        "price_1, latency_1, _ = call_model(HARD_QUERY, 'hard')\n",
        "price_2, latency_2, _ = call_model(EASY_QUERY, 'hard')\n",
        "\n",
        "tot_price = price_1 + price_2\n",
        "tot_latency = latency_1 + latency_2\n",
        "\n",
        "print(\"\\n-------------------------------\")\n",
        "print(\"TOTAL PRICE:\", tot_price)\n",
        "print(\"TOTAL LATENCY:\", tot_latency)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2u0Sl1d0_a4",
        "outputId": "e82a2f26-3ec3-4f03-d10c-900a4dd5a724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using ONLY powerful model (gpt-4o mock):\n",
            "\n",
            "[MOCK RESPONSE from HARD model]\n",
            "[MOCK RESPONSE from HARD model]\n",
            "\n",
            "-------------------------------\n",
            "TOTAL PRICE: 995\n",
            "TOTAL LATENCY: 0.4585906607335248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def router(QUERY):\n",
        "    # Сначала вызываем классификатор\n",
        "    price_intent, latency_intent, intent = call_model(QUERY, 'intent')\n",
        "\n",
        "    # Маршрутизация\n",
        "    if intent.strip().lower() == 'hard':\n",
        "        price_answer, latency_answer, _ = call_model(QUERY, 'hard')\n",
        "    else:\n",
        "        price_answer, latency_answer, _ = call_model(QUERY, 'easy')\n",
        "\n",
        "    total_price = price_intent + price_answer\n",
        "    total_latency = latency_intent + latency_answer\n",
        "\n",
        "    print(f\"\\nROUTED AS: {intent}\")\n",
        "    print(f\"Total price: {total_price}\")\n",
        "    print(f\"Total latency: {total_latency}\")\n",
        "\n",
        "    return total_price, total_latency\n"
      ],
      "metadata": {
        "id": "hJkhiiWM1HNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MULTI-MODEL SYSTEM:\\n\")\n",
        "\n",
        "price_hard, latency_hard = router(HARD_QUERY)\n",
        "print(\"\\n----------------------------------\\n\")\n",
        "price_easy, latency_easy = router(EASY_QUERY)\n",
        "\n",
        "total_multi_price = price_hard + price_easy\n",
        "total_multi_latency = latency_hard + latency_easy\n",
        "\n",
        "print(\"\\n==================================\")\n",
        "print(\"TOTAL MULTI-MODEL PRICE:\", total_multi_price)\n",
        "print(\"TOTAL MULTI-MODEL LATENCY:\", total_multi_latency)\n",
        "print(\"==================================\")\n"
      ],
      "metadata": {
        "id": "z-iiZ07b1Orr",
        "outputId": "eb200037-4018-49a2-cf3d-49c3613fefcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MULTI-MODEL SYSTEM:\n",
            "\n",
            "hard\n",
            "[MOCK RESPONSE from HARD model]\n",
            "\n",
            "ROUTED AS: hard\n",
            "Total price: 294.5\n",
            "Total latency: 0.32482900179219476\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "simple\n",
            "[MOCK RESPONSE from EASY model]\n",
            "\n",
            "ROUTED AS: simple\n",
            "Total price: 98.5\n",
            "Total latency: 0.3085622539791131\n",
            "\n",
            "==================================\n",
            "TOTAL MULTI-MODEL PRICE: 393.0\n",
            "TOTAL MULTI-MODEL LATENCY: 0.6333912557713078\n",
            "==================================\n"
          ]
        }
      ]
    }
  ]
}